<!DOCTYPE html>
<html>
<head>
        <title>Welcome to Harlei"s Blog!</title>
</head>
<body style="background-color: bisque;">
    <center>
    <div class="container">
        <div class="image">
            <img src="https://github.com/harleicustodio/MyBlog/blob/main/fb_pic.jpg"
            width="300"
            height="400" />
        </div>
        <div class="text">
            <h1>Meet the Author</h1>
            <h3>Hey, I'm Harlei F. Custodio! I'm currently studying as a first year in college and starting out as a coder. I'm a Computer Science major with <br> a specialization in Data Science in the University of Perpetual Help - Molino Campus.</h3>
        </div>
    </div>
    </center>
    <center>
        <hr width="50%" color="black" size="3px" />
        <h1>Ever wonder how computing started?</h1>
        <h2>The History of Computing.</h2>
</center>
     <center>
        <h2>Early Beginnings</h2>
        <p align="justify">
                <center>
                <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/abacus.jpg"
                    width="600"
                    height="420" />
                </div>
                </center>
            <center>
                        An abacus is a simple manual calculating tool that has been used for thousands of years. It consists of a frame <br>
                        with rows of beads or counters that can be moved back and forth to represent numbers. The exact date of the abacus's <br>
                        invention is unknown, but it is believed to have originated in ancient Mesopotamia or China. Some historians trace its <br>
                        roots back to the 3rd or 4th millennium BC. There is no single inventor credited with the abacus. It likely evolved over <br>
                        time through various cultures and civilizations.
            </center>
        </p>
     </center>
     <center>
        <p align="justify">
                <center>
                <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/anti.jpg"
                    width="220"
                    height="200" />
                </div>
                </center>
             <center>
                        The Antikythera Mechanism is an ancient Greek astronomical calculator, considered the most complex piece of engineering <br>
                        from its era. It was discovered in 1901 in a shipwreck off the Greek island of Antikythera. Dated to the 2nd century BC,<br> 
                        it is believed to have been invented by Greek scientists and engineers. The mechanism is a complex system of gears and dials <br>
                        that could predict astronomical events, such as eclipses, planetary positions, and the cycles of the Olympic Games. Its intricate <br>
                        design and advanced functionality have led many to consider it as a precursor to modern mechanical clocks and computers.
             </center>
        </p>
      </center>
      <center>
        <h2>19 Century</h2>
        <p align="justify">
                <center>
                <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/engine.jpg"
                    width="400"
                    height="400" />
                </div>
                </center>
              <center>
                        The Analytical Engine was a proposed mechanical general-purpose computer designed by Charles Babbage in the 1830s. It was <br>
                        intended to be a programmable machine capable of performing a wide range of calculations. While Babbage never completed a <br>
                        fully functional model due to technological limitations and funding issues, the Analytical Engine is considered a significant <br>
                        milestone in the history of computing. It laid the groundwork for modern computers by introducing concepts such as programmability, <br>
                        memory, and the use of punched cards for input and output.
             </center>
        </p>
     </center>
     <center>
        <p align="justify">
                <center>
                <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/ada.jpg"
                    width="750"
                    height="400" />
                </div>
                </center>
             <center>
                        Ada Lovelace was a 19th-century English mathematician and writer who is considered the world's first computer programmer. <br> 
                        She is known for her work on Charles Babbage's Analytical Engine, a mechanical general-purpose computer. Lovelace wrote <br> 
                        the first algorithm intended to be processed by a machine, a method for calculating Bernoulli numbers. This algorithm is <br>
                        considered to be a foundational piece of software development and has earned her the title of "the first computer programmer." <br>
                        Her contributions to the field of computing were groundbreaking for her time and have had a lasting impact on the development of <br>
                        modern computers. Lovelace's work helped to establish the concept of a machine capable of following a sequence of instructions, <br>
                        laying the foundation for the development of programmable computers.
             </center>
        </p>
     </center>
     <center>
        <h2>Early 20th Century</h2>
          <p align="justify">
                  <center>
                  <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/turing.jpg"
                    width="550"
                    height="250" />
                  </div>
                  </center>
                <center>
                        A Turing machine is a theoretical computing device that is used to model a general-purpose computer. It was introduced by <br>
                        British mathematician Alan Turing in 1936 in his paper "On Computable Numbers, with an Application to the Entscheidungsproblem." <br>
                        Turing machines are often used to study the limits of computation and to understand the nature of algorithms. They are a fundamental <br>
                        concept in computer science and theoretical computer science. While Turing machines are not physical devices, they serve as a <br>
                        mathematical model that can be used to analyze the capabilities and limitations of real-world computers.
                </center>
           </p>
        </center>
        <center>
          <p align="justify">
                  <center>
                  <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/eniac.jpg"
                    width="600"
                    height="410" />
                  </div>
                  </center>
                <center>
                        ENIAC stands for Electronic Numerical Integrator and Computer. It was one of the earliest electronic general-purpose computers, <br>
                        developed during World War II. It was invented in 1946 by a team of engineers and scientists at the University of Pennsylvania, <br> 
                        led by John Mauchly and J. Presper Eckert. ENIAC was a massive machine, weighing over 30 tons and occupying a large room. It used <br> 
                        vacuum tubes for computation and was primarily designed to calculate artillery firing tables for the U.S. Army.
                </center>
           </p>
        </center>
        <center>
        <h2>Mid 20th Century</h2>
           <p align="justify">
                   <center>
                   <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/transistor.jpg"
                    width="470"
                    height="420" />
                   </div>
                   </center>
                <center>
                        A transistor is a semiconductor device used to amplify or switch electronic signals. It is the fundamental building block of modern <br> 
                        electronic circuits. The transistor was invented in 1947 by John Bardeen, Walter Brattain, and William Shockley at Bell Labs. Their <br>
                        invention revolutionized electronics, leading to smaller, more powerful, and more reliable devices. Transistors replaced vacuum tubes, <br>
                        which were large, bulky, and prone to failure.
                </center>
           </p>
        </center>
        <center>
          <p align="justify">
                  <center>
                  <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/inte.jpeg"
                    width="600"
                    height="400" />
                  </div>
                  </center>
                <center>
                        An integrated circuit (IC), also known as a chip or microchip, is a miniature electronic circuit that contains thousands or even millions <br>
                        of transistors and other electronic components on a single piece of semiconductor material. The first integrated circuits were invented in <br>
                        the late 1950s by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor. The invention of the integrated circuit <br>
                        revolutionized electronics by making it possible to pack more components into a smaller space and reduce manufacturing costs. This led <br>
                        to the development of smaller, more powerful, and more affordable electronic devices.
                </center>
           </p>
        </center>
        <center>
        <h2>Late 20th Century</h2>
          <p align="justify">
                  <center>
                  <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/altair.jfif"
                    width="600"
                    height="400" />
                  </div>
                  </center>
                <center>
                        Personal computers (PCs) are computers designed for individual use, typically smaller and less powerful than mainframes or minicomputers. <br>
                        They are used for a wide range of tasks, including word processing, spreadsheets, gaming, web browsing, and more. The first personal <br>
                        computers were introduced in the 1970s. The Altair 8800, released in 1975, is often considered one of the earliest commercially available <br>
                        personal computers. Several companies contributed to the development and popularization of personal computers. Some of the most notable include: <br>
                        IBM: IBM introduced the IBM PC in 1981, which became a huge success and set the standard for personal computers for many years. <br>
                        Apple: Apple Computer, founded by Steve Jobs and Steve Wozniak, introduced the Apple II in 1977. The Apple II was one of the first personal <br>
                        computers to be widely adopted for home use. Commodore: Commodore Business Machines produced a series of popular personal computers, including <br>
                        the Commodore 64 and the Amiga. These are just a few examples of the many companies involved in the development and manufacturing of personal <br>
                        computers. Today, personal computers are ubiquitous and have become an essential tool for individuals and businesses alike.
                </center>
          </p>
        </center>
        <center>
          <p align="justify">
                  <center>
                  <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/internet.jpg"
                    width="800"
                    height="400" />
                  </div>
                  </center>
                <center>
                        The internet is a global network of interconnected computers that allows users to access and share information. It is a vast digital space where <br>
                        people can communicate, conduct business, and access a wide range of resources. The origins of the internet can be traced back to the 1960s. The <br>
                        ARPANET (Advanced Research Projects Agency Network), a network funded by the U.S. Department of Defense, was developed as a way to connect research <br>
                        computers across the country. There is no single inventor of the internet. It was a collaborative effort involving many researchers, engineers, and <br> 
                        organizations. Some of the key figures involved in its development include: Vinton Cerf: Often referred to as one of the "fathers of the internet," <br>
                        Cerf played a crucial role in developing the TCP/IP protocols that form the foundation of the internet. Robert Kahn: Another "father of the internet," <br>
                        Kahn collaborated with Cerf on the development of TCP/IP. Lawrence Roberts: Roberts was a key figure in the development of ARPANET and the early internet. <br>
                        Over time, ARPANET evolved into the internet, which has grown exponentially in size and complexity. Today, it is an essential part of our daily lives, <br>
                        providing access to information, communication, and entertainment.
                </center>
           </p>
        </center>
        <center>
          <p align="justify">
                  <center>
                  <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/www.jpg"
                    width="800"
                    height="450" />
                  </div>
                  </center>
                <center>
                        WWW stands for World Wide Web. It is a system of interconnected hypertext documents that are accessed through the internet. The WWW allows users to <br>
                        navigate from one document to another by clicking on hyperlinks. The WWW was invented in 1989 by Tim Berners-Lee, a British computer scientist working <br>
                        at CERN, the European Organization for Nuclear Research. Berners-Lee developed the HTML (HyperText Markup Language) language and the HTTP (HyperText <br>
                        Transfer Protocol) protocol, which together form the foundation of the WWW. The invention of the WWW transformed the way people access and share information, <br>
                        making it easier for individuals and organizations to publish and consume content online.
                </center>
           </p>
        </center>
        <center>
        <h2>21st Century</h2>
           <p align="justify">
                   <center>
                   <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/smart.jpg"
                    width="900"
                    height="450" />
                   </div>
                   </center>
                <center>
                        Smartphones are a type of mobile phone that combines the functionality of a traditional phone with features like a touchscreen, internet connectivity, <br>
                        and the ability to run applications. They are essentially miniaturized computers that fit in your pocketThe exact date of the invention of the smartphone is <br> 
                        debatable. However, the Palm Pilot, released in 1996, is often considered one of the earliest smartphones. It had a touchscreen and could run basic <br>
                        applications, but it lacked internet connectivity. The iPhone, released by Apple in 2007, is widely credited with popularizing the modern smartphone. It <br>
                        introduced a multi-touch interface, a large touchscreen, and the ability to run third-party applications from the App Store. The iPhone's success led to <br>
                        a surge in the popularity of smartphones and the development of competing devices from other manufacturers. While there is no single inventor of the smartphone, <br>
                        many companies and individuals contributed to its development. Apple's iPhone played a significant role in shaping the modern smartphone landscape, but other <br>
                        manufacturers, such as Samsung and Google, have also made significant contributions.
                </center>
           </p>
        </center>
        <center>
           <p align="justify">
                   <center>
                   <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/quantum.jpg"
                    width="770"
                    height="510" />
                   </div>
                   </center>
                <center>
                        Quantum computing is a type of computing that leverages the principles of quantum mechanics to perform calculations that would be impractical or impossible <br>
                        for classical computers. It uses quantum bits, or qubits, which can exist in multiple states simultaneously, allowing for parallel processing and potentially <br>
                        solving complex problems that are intractable for classical computers. While the theoretical concepts of quantum computing were explored in the mid-20th century, <br>
                        the development of practical quantum computers is a relatively recent endeavor. The first small-scale quantum computers were developed in the late 20th and early <br>
                        21st centuries, and research and development in this field are ongoing. There is no single inventor of quantum computing. It is the result of decades of research <br>
                        and development by many scientists and engineers. Some of the key figures in the field include: Paul Benioff: Proposed the concept of a quantum Turing machine.<br>
                        Richard Feynman: Suggested that quantum mechanics could be used to simulate quantum systems. Peter Shor: Developed Shor's algorithm, which could be used to factor <br>
                        large numbers efficiently on a quantum computer. David Deutsch: Proposed the quantum circuit model. Today, quantum computing is a rapidly evolving field with the <br>
                        potential to revolutionize various industries, including drug discovery, materials science, and cryptography.
                </center>
            </p>
        </center>
        <center>
        <h1>Introduction to IT Basics</h1>
           <p align="justify">
                   <center>
                   <div class="image">
                    <img src="https://github.com/harleicustodio/MyBlog/blob/main/it.jpg"
                    width="490"
                    height="280" />
                   </div>
                   </center>
                <center>
                        Hardware refers to the physical components of a computer system. These are the tangible parts that you can see, touch, and interact with. Hardware includes everything <br>
                        from the external components like the keyboard, mouse, and monitor, to the internal components such as the motherboard, processor, RAM, hard drive, and graphics card. <br>
                        Think of hardware as the skeleton and muscles of a computer, providing the foundation for the software to run. Software is the intangible part of a computer system that <br>
                        consists of instructions or programs. It's the brain of the computer, responsible for controlling the hardware and performing specific tasks. Think of software as the <br>
                        applications and operating systems that you interact with on a daily basis. Examples of software include word processing programs, web browsers, games, and the operating <br>
                        system itself. Software tells the hardware what to do and how to do it. Networking refers to the interconnection of multiple computers or devices to share resources, communicate, <br>
                        and exchange data. It involves connecting these devices using cables, wireless connections, or other means, and establishing rules and protocols for how they should interact. <br>
                        Networks can range from small local networks (LANs) within a home or office to vast global networks like the internet. Networking is essential for modern computing, enabling us <br>
                        to access information, communicate with others, and collaborate on projects. IT (Information Technology) plays a pivotal role in modern organizations, serving as the backbone of <br>
                        operations and driving innovation. IT departments are responsible for managing and maintaining the organization's technological infrastructure, including hardware, software, networks, <br>
                        and data. They ensure that technology is used effectively to improve efficiency, productivity, and competitiveness. IT professionals also play a crucial role in implementing new <br>
                        technologies, developing innovative solutions, and safeguarding sensitive data. In today's digital age, IT has become an indispensable asset for organizations of all sizes, <br> 
                        enabling them to adapt to changing market conditions and stay ahead of the competition.
                </center>
            </p>
        </center>
</body>
</html>
