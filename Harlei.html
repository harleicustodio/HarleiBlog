<!DOCTYPE html>
<html>
<head>
        <title>Welcome to Harlei"s Blog!</title>
</head>
<body style="background-color: bisque;">
    <center>
    <div class="container">
        <div class="image"> 
            <img src="'fbpic.jpg"
            width="300"
            height="400" />
        </div>
        <div class="text">
            <h1>Meet the Author</h1>
            <h3>Hey, I'm Harlei F. Custodio! I'm currently studying as a first year in college and starting out as a coder. I'm a Computer Science major with <br> a specialization in Data Science in the University of Perpetual Help - Molino Campus.</h3>
        </div>
    </div>
    </center>
    <center>
        <hr width="50%" color="black" size="3px" />
        <h1>Ever wonder how computing started?</h1>
        <h2>The Histody of Computing.</h2>
    </center>
    <center>
<h2>Early Beginnings</h2>
        <p align="justify">
            <center>
            An abacus is a simple manual calculating tool that has been used for thousands of years. It consists of a frame with rows of beads or counters that can be moved back and forth to represent numbers. <br>
The exact date of the abacus's invention is unknown, but it is believed to have originated in ancient Mesopotamia or China. Some historians trace its roots back to the 3rd or 4th millennium BC. <br>
There is no single inventor credited with the abacus. It likely evolved over time through various cultures and civilizations.
            </center>
        </p>
    </center>
</body>
<center>
<p align="justify">
<center>
The Antikythera Mechanism is an ancient Greek astronomical calculator, considered the most complex piece of engineering from its era. It was discovered in 1901 in a shipwreck off the Greek island of Antikythera. <br>
Dated to the 2nd century BC, it is believed to have been invented by Greek scientists and engineers. The mechanism is a complex system of gears and dials that could predict astronomical events, such as eclipses, planetary positions, and the cycles of the Olympic Games. <br>
Its intricate design and advanced functionality have led many to consider it as a precursor to modern mechanical clocks and computers.
</center>
</p>
</center>
<center>
<h2>19 Century</h2>
<p align="justify">
<center>
The Analytical Engine was a proposed mechanical general-purpose computer designed by Charles Babbage in the 1830s. It was intended to be a programmable machine capable of performing a wide range of calculations. <br>
While Babbage never completed a fully functional model due to technological limitations and funding issues, the Analytical <br> Engine is considered a significant milestone in the history of computing. It laid the groundwork for modern computers by introducing concepts such as programmability, memory, and the use of punched cards for input and output.
</center>
</p>
</center>
<center>
<p align="justify">
<center>
Ada Lovelace was a 19th-century English mathematician and writer who is considered the world's first computer programmer. She is known for her work on Charles Babbage's Analytical Engine, a mechanical general-purpose computer. <br>
Lovelace wrote the first algorithm intended to be processed by a machine, a method for calculating Bernoulli numbers. This algorithm is considered to be a foundational piece of software development and has earned her the title of "the first computer programmer."
<br>
Her contributions to the field of computing were groundbreaking for her time and have had a lasting impact on the development of modern computers. Lovelace's work helped to establish the concept of a machine capable of following a sequence of instructions, laying the foundation for the development of programmable computers.
</center>
</p>
</center>
<center>
<h2>Early 20th Century</h2>
<p align="justify">
<center>
A Turing machine is a theoretical computing device that is used to model a general-purpose computer. It was introduced by British mathematician Alan Turing in 1936 in his paper "On Computable Numbers, with an Application to the Entscheidungsproblem." <br>
Turing machines are often used to study the limits of computation and to understand the nature of algorithms. They are a fundamental concept in computer science and theoretical computer science. <br>
While Turing machines are not physical devices, they serve as a mathematical model that can be used to analyze the capabilities and limitations of real-world computers.
</center>
</p>
</center>
<center>
<p align="justify">
<center>
ENIAC stands for Electronic Numerical Integrator and Computer. It was one of the earliest electronic general-purpose computers, developed during World War II.  <br> 
It was invented in 1946 by a team of engineers and scientists at the University of Pennsylvania, led by John Mauchly and J. Presper Eckert. <br>
ENIAC was a massive machine, weighing over 30 tons and occupying a large room. It used vacuum tubes for computation and was primarily designed to calculate artillery firing tables for the U.S. Army.
</center>
</p>
</center>
<center>
<h2>Mid 20th Century</h2>
<p align="justify">
<center>
A transistor is a semiconductor device used to amplify or switch electronic signals. It is the fundamental building block of modern electronic circuits.
<br>
The transistor was invented in 1947 by John Bardeen, Walter Brattain, and William Shockley at Bell Labs.
<br>
Their invention revolutionized electronics, leading to smaller, more powerful, and more reliable devices. Transistors replaced vacuum tubes, which were large, bulky, and prone to failure.
</center>
</p>
</center>
<center>
<p align="justify">
<center>
An integrated circuit (IC), also known as a chip or microchip, is a miniature electronic circuit that contains thousands or even millions of transistors and other electronic components on a single piece of semiconductor material. <br>
The first integrated circuits were invented in the late 1950s by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor. <br>
The invention of the integrated circuit revolutionized electronics by making it possible to pack more components into a smaller space and reduce manufacturing costs. This led to the development of smaller, more powerful, and more affordable electronic devices.
</center>
</p>
</center>
<center>
<h2>Late 20th Century</h2>
<p align="justify">
<center>
Personal computers (PCs) are computers designed for individual use, typically smaller and less powerful than mainframes or minicomputers. They are used for a wide range of tasks, including word processing, spreadsheets, gaming, web browsing, and more.
<br>
The first personal computers were introduced in the 1970s. The Altair 8800, released in 1975, is often considered one of the earliest commercially available personal computers.
<br>
Several companies contributed to the development and popularization of personal computers. Some of the most notable include:
<br>
IBM: IBM introduced the IBM PC in 1981, which became a huge success and set the standard for personal computers for many years. <br>
Apple: Apple Computer, founded by Steve Jobs and Steve Wozniak, introduced the Apple II in 1977. The Apple II was one of the first personal computers to be widely adopted for home use. <br>
Commodore: Commodore Business Machines produced a series of popular personal computers, including the Commodore 64 and the Amiga. <br>
These are just a few examples of the many companies involved in the development and manufacturing of personal computers. <br> Today, personal computers are ubiquitous and have become an essential tool for individuals and businesses alike.
</center>
</p>
</center>
<center>
<p align="justify">
<center>
The internet is a global network of interconnected computers that allows users to access and share information. It is a vast digital space where people can communicate, conduct business, and access a wide range of resources.
<br>
The origins of the internet can be traced back to the 1960s. The ARPANET (Advanced Research Projects Agency Network), a network funded by the U.S. Department of Defense, was developed as a way to connect research computers across the country.
<br>
There is no single inventor of the internet. It was a collaborative effort involving many researchers, engineers, and organizations. Some of the key figures involved in its development include:
<br>
Vinton Cerf: Often referred to as one of the "fathers of the internet," Cerf played a crucial role in developing the TCP/IP protocols that form the foundation of the internet.
<br>
Robert Kahn: Another "father of the internet," Kahn collaborated with Cerf on the development of TCP/IP.
<br>
Lawrence Roberts: Roberts was a key figure in the development of ARPANET and the early internet.
<br>
Over time, ARPANET evolved into the internet, which has grown exponentially in size and complexity. Today, it is an essential part of our daily lives, providing access to information, communication, and entertainment.
</center>
</p>
</center>
<center>
<p align="justify">
<center>
WWW stands for World Wide Web. It is a system of interconnected hypertext documents that are accessed through the internet. The WWW allows users to navigate from one document to another by clicking on hyperlinks.
<br>
The WWW was invented in 1989 by Tim Berners-Lee, a British computer scientist working at CERN, the European Organization for Nuclear Research. Berners-Lee developed the HTML (HyperText Markup Language) language and the HTTP (HyperText Transfer Protocol) protocol, which together form the foundation of the WWW.   
<br>
The invention of the WWW transformed the way people access and share information, making it easier for individuals and organizations to publish and consume content online.
</center>
</p>
</center>
<center>
<h2>21st Century</h2>
<p align="justify">
<center>
Smartphones are a type of mobile phone that combines the functionality of a traditional phone with features like a<br> touchscreen, internet connectivity, and the ability to run applications. They are essentially miniaturized computers that fit in your pocket.
<br>
The exact date of the invention of the smartphone is debatable. However, the Palm Pilot, released in 1996, is often considered one of the earliest smartphones. It had a<br> touchscreen and could run basic applications, but it lacked internet connectivity.
<br>
The iPhone, released by Apple in 2007, is widely credited with popularizing the modern smartphone. It introduced a multi-touch interface, a large touchscreen, and the<br> ability to run third-party applications from the App Store. The iPhone's success led to a surge in the popularity of smartphones and the development of competing devices from other manufacturers.
<br>
While there is no single inventor of the smartphone, many companies and individuals contributed to its development. Apple's iPhone played a significant role in<br> shaping the modern smartphone landscape, but other manufacturers, such as Samsung and Google, have also made significant contributions.
</center>
</p>
</center>
<center>
<p align="justify">
<center>
Quantum computing is a type of computing that leverages the principles of quantum mechanics to perform calculations that would be impractical or impossible for classical computers. It uses quantum bits, or qubits, which can exist in multiple states simultaneously, allowing for parallel processing and potentially solving complex problems that are intractable for classical computers.   
<br>
While the theoretical concepts of quantum computing were explored in the mid-20th century, the development of practical quantum computers is a relatively recent endeavor. The first small-scale quantum computers were developed in the late 20th and early 21st centuries, and research and development in this field are ongoing.
<br>
There is no single inventor of quantum computing. It is the result of decades of research and development by many scientists and engineers. Some of the key figures in the field include:
<br>
Paul Benioff: Proposed the concept of a quantum Turing machine.<br>
Richard Feynman: Suggested that quantum mechanics could be used to simulate quantum systems.<br>
Peter Shor: Developed Shor's algorithm, which could be used to factor large numbers efficiently on a quantum computer.<br>
David Deutsch: Proposed the quantum circuit model. <br>
Today, quantum computing is a rapidly evolving field with the potential to revolutionize various industries, including drug discovery, materials science, and cryptography.
</center>
</p>
</center>
<center>
<h1>Introduction to IT Basics</h1>
<p align="justify">
<center>
Hardware refers to the physical components of a computer system. These are the tangible parts that you can see, touch, and interact with. Hardware includes everything from the external components like the keyboard, mouse, and monitor, to the<br> internal components such as the motherboard, processor, RAM, hard drive, and graphics card. Think of hardware as the skeleton and muscles of a computer, providing the foundation for the software to run.
<br>
Software is the intangible part of a computer system that consists of instructions or programs. It's the brain of the computer, responsible for controlling the hardware and performing specific tasks. Think of software as the applications and operating systems that you interact with on a daily basis.<br> Examples of software include word processing programs, web browsers, games, and the operating system itself. Software tells the hardware what to do and how to do it.
<br>
Networking refers to the interconnection of multiple computers or devices to share resources, communicate, and exchange data. It involves connecting these devices using cables, wireless connections, or other means, and establishing rules and protocols for how they should interact. Networks can range from small local networks (LANs)<br> within a home or office to vast global networks like the internet. Networking is essential for modern computing, enabling us to access information, communicate with others, and collaborate on projects.
<br>
IT (Information Technology) plays a pivotal role in modern organizations, serving as the backbone of operations and driving innovation. IT departments are responsible for managing and maintaining the<br> organization's technological infrastructure, including hardware, software, networks, and data. They ensure that technology is used effectively to improve efficiency,<br> productivity, and competitiveness. IT professionals also play a crucial role in implementing new technologies, developing innovative solutions, and safeguarding sensitive data. In today's digital age, IT has become an indispensable asset for<br> organizations of all sizes, enabling them to adapt to changing market conditions and stay ahead of the competition.
</center>
</p>
</center>
</body>
</html>